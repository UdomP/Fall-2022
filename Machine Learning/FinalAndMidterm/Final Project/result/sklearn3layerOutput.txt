Iteration 1, loss = 43.38135129
Iteration 2, loss = 8.03058572
Iteration 3, loss = 4.59001046
Iteration 4, loss = 3.96749501
Iteration 5, loss = 3.34556731
Iteration 6, loss = 2.88562335
Iteration 7, loss = 2.54783296
Iteration 8, loss = 2.31476902
Iteration 9, loss = 2.28337787
Iteration 10, loss = 2.60381865
Iteration 11, loss = 2.99897372
Iteration 12, loss = 2.90303509
Iteration 13, loss = 2.54912671
Iteration 14, loss = 1.94949262
Iteration 15, loss = 1.92061004
Iteration 16, loss = 1.94085489
Iteration 17, loss = 1.78345001
Iteration 18, loss = 1.75050564
Iteration 19, loss = 1.67957025
Iteration 20, loss = 1.66248561
Iteration 21, loss = 1.64583385
Iteration 22, loss = 1.71388783
Iteration 23, loss = 1.63856673
Iteration 24, loss = 1.57749779
Iteration 25, loss = 1.53714104
Iteration 26, loss = 1.52237747
Iteration 27, loss = 1.52671276
Iteration 28, loss = 1.63624817
Iteration 29, loss = 1.58482681
Iteration 30, loss = 1.46540148
Iteration 31, loss = 1.47271756
Iteration 32, loss = 1.45065723
Iteration 33, loss = 1.43929776
Iteration 34, loss = 1.41739747
Iteration 35, loss = 1.43799365
Iteration 36, loss = 1.44316979
Iteration 37, loss = 1.38986532
Iteration 38, loss = 1.37569727
Iteration 39, loss = 1.35477047
Iteration 40, loss = 1.38630192
Iteration 41, loss = 1.42999360
Iteration 42, loss = 1.33208175
Iteration 43, loss = 1.33546085
Iteration 44, loss = 1.36944551
Iteration 45, loss = 1.33747606
Iteration 46, loss = 1.34576643
Iteration 47, loss = 1.44862898
Iteration 48, loss = 1.39021625
Iteration 49, loss = 1.34293622
Iteration 50, loss = 1.39323145
Iteration 51, loss = 1.25553592
Iteration 52, loss = 1.25657414
Iteration 53, loss = 1.24477922
Iteration 54, loss = 1.24096440
Iteration 55, loss = 1.21538908
Iteration 56, loss = 1.24326754
Iteration 57, loss = 1.25946422
Iteration 58, loss = 1.19517350
Iteration 59, loss = 1.18819979
Iteration 60, loss = 1.20251255
Iteration 61, loss = 1.18791392
Iteration 62, loss = 1.23605189
Iteration 63, loss = 1.16297958
Iteration 64, loss = 1.16296251
Iteration 65, loss = 1.16553912
Iteration 66, loss = 1.14451162
Iteration 67, loss = 1.14758626
Iteration 68, loss = 1.16300106
Iteration 69, loss = 1.38799299
Iteration 70, loss = 1.50779185
Iteration 71, loss = 1.34193291
Iteration 72, loss = 1.64429568
Iteration 73, loss = 1.66958997
Iteration 74, loss = 1.18007715
Iteration 75, loss = 1.10020500
Iteration 76, loss = 1.09419547
Iteration 77, loss = 1.10257122
Iteration 78, loss = 1.14987069
Iteration 79, loss = 1.07473010
Iteration 80, loss = 1.06672245
Iteration 81, loss = 1.07859269
Iteration 82, loss = 1.06494746
Iteration 83, loss = 1.05557784
Iteration 84, loss = 1.05438988
Iteration 85, loss = 1.03866098
Iteration 86, loss = 1.04773219
Iteration 87, loss = 1.03985647
Iteration 88, loss = 1.06571218
Iteration 89, loss = 1.13073582
Iteration 90, loss = 1.02812936
Iteration 91, loss = 1.00612655
Iteration 92, loss = 1.01530582
Iteration 93, loss = 0.99545208
Iteration 94, loss = 1.00402975
Iteration 95, loss = 1.00618768
Iteration 96, loss = 0.98630071
Iteration 97, loss = 0.98895513
Iteration 98, loss = 0.99138326
Iteration 99, loss = 1.00827990
Iteration 100, loss = 0.97356517
C:\Users\serey\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.
  warnings.warn(
11 10.65036578422648
0 -1.1162333887792086
11 10.327658665120781
9 8.693522877470272
8 7.925764587725954
14 13.315420592842766
17 16.975313710217094
15 14.809892092370022
18 18.47350791999297
10 9.047447738105548
15 14.0280404887009
10 8.392647131786688
14 12.996306165781553
12 11.00619812821184
11 6.286901512328991
0 3.9031388802597307
12 12.288671730711428
10 9.916024643665121
11 12.01852475604162
5 5.825322331737681
15 16.255619879244733
14 11.185711805591367
12 12.05669905795634
15 15.573963170947742
16 14.525233004127468
16 14.74693954621349
0 -0.17595678191539468
15 15.085733354080023
8 8.173444174831623
10 9.382267346902761
12 12.193463918275658
15 14.879656083766255
18 17.0542235062177
10 10.032549481093264
10 9.593891985790789
0 2.751199066144316
13 13.412998836612612
18 17.45371853133494
11 11.57632587702074
11 10.8457862851035
MSE:  1.7966916340451655