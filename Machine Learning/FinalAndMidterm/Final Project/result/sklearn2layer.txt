Iteration 1, loss = 33.96334502
Iteration 2, loss = 3.65152762
Iteration 3, loss = 2.88520355
Iteration 4, loss = 2.68831760
Iteration 5, loss = 2.59549535
Iteration 6, loss = 2.51030965
Iteration 7, loss = 2.53630700
Iteration 8, loss = 2.17972495
Iteration 9, loss = 2.08788229
Iteration 10, loss = 1.99042187
Iteration 11, loss = 1.92606057
Iteration 12, loss = 1.89338929
Iteration 13, loss = 1.81838158
Iteration 14, loss = 1.78495255
Iteration 15, loss = 1.75840139
Iteration 16, loss = 1.72023034
Iteration 17, loss = 1.69353462
Iteration 18, loss = 1.70965866
Iteration 19, loss = 1.85503019
Iteration 20, loss = 1.71674508
Iteration 21, loss = 1.65120591
Iteration 22, loss = 1.63212728
Iteration 23, loss = 1.56979767
Iteration 24, loss = 1.54988279
Iteration 25, loss = 1.57172946
Iteration 26, loss = 1.55214335
Iteration 27, loss = 1.51143334
Iteration 28, loss = 1.49444791
Iteration 29, loss = 1.47840082
Iteration 30, loss = 1.48783977
Iteration 31, loss = 1.48155294
Iteration 32, loss = 1.43793175
Iteration 33, loss = 1.41737243
Iteration 34, loss = 1.40540395
Iteration 35, loss = 1.40041205
Iteration 36, loss = 1.39240233
Iteration 37, loss = 1.38822887
Iteration 38, loss = 1.57734340
Iteration 39, loss = 1.56132135
Iteration 40, loss = 1.34305776
Iteration 41, loss = 1.33572857
Iteration 42, loss = 1.31362046
Iteration 43, loss = 1.30391485
Iteration 44, loss = 1.30662059
Iteration 45, loss = 1.34223784
Iteration 46, loss = 1.29938014
Iteration 47, loss = 1.27794649
Iteration 48, loss = 1.30614951
Iteration 49, loss = 1.32255289
Iteration 50, loss = 1.25344436
Iteration 51, loss = 1.34381995
Iteration 52, loss = 1.31579359
Iteration 53, loss = 1.23236476
Iteration 54, loss = 1.22250785
Iteration 55, loss = 1.22865001
Iteration 56, loss = 1.21585784
Iteration 57, loss = 1.20432970
Iteration 58, loss = 1.22801635
Iteration 59, loss = 1.18221894
Iteration 60, loss = 1.19528145
Iteration 61, loss = 1.18922749
Iteration 62, loss = 1.24823182
Iteration 63, loss = 1.31653123
Iteration 64, loss = 1.16742745
Iteration 65, loss = 1.15846145
Iteration 66, loss = 1.22291216
Iteration 67, loss = 1.28311546
Iteration 68, loss = 1.13467401
Iteration 69, loss = 1.13138516
Iteration 70, loss = 1.23247016
Iteration 71, loss = 1.25247799
Iteration 72, loss = 1.11390857
Iteration 73, loss = 1.11309805
Iteration 74, loss = 1.14375168
Iteration 75, loss = 1.15567067
Iteration 76, loss = 1.35343355
Iteration 77, loss = 1.12417352
Iteration 78, loss = 1.09173930
Iteration 79, loss = 1.07444316
Iteration 80, loss = 1.07080256
Iteration 81, loss = 1.14172767
Iteration 82, loss = 1.23444242
Iteration 83, loss = 1.09369576
Iteration 84, loss = 1.04521891
Iteration 85, loss = 1.04820133
Iteration 86, loss = 1.03497558
Iteration 87, loss = 1.03493414
Iteration 88, loss = 1.04805558
Iteration 89, loss = 1.03029116
Iteration 90, loss = 1.01945294
Iteration 91, loss = 1.01590514
Iteration 92, loss = 1.03325023
Iteration 93, loss = 1.01411934
Iteration 94, loss = 1.01502550
Iteration 95, loss = 1.01045658
Iteration 96, loss = 1.05505733
Iteration 97, loss = 0.99116590
Iteration 98, loss = 1.09038636
Iteration 99, loss = 1.07215721
Iteration 100, loss = 1.00220954
C:\Users\serey\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.
  warnings.warn(
11 10.736657193490668
0 -0.38256421628906473
11 10.686564315523542
9 9.624287777552132
8 8.280308605198233
14 13.818874383280983
17 17.453494490370762
15 14.792960830472557
18 18.702486072020793
10 9.272656126343144
15 14.017862825454557
10 8.889334385137385
14 13.184253826882232
12 10.034805532910054
11 6.58144156742021
0 4.381681517281274
12 11.311794230187614
10 10.093354723075105
11 12.685641318543555
5 5.806374620449365
15 16.30508926167744
14 11.896248567470474
12 12.440249103597914
15 16.02288463320897
16 14.933364988979514
16 16.14815649475884
0 -0.2620067491446438
15 15.038318276552424
8 8.691195027194393
10 8.716096430595755
12 13.071614347527651
15 15.740694489061196
18 17.488855429045
10 10.720556920034303
10 9.596062252416575
0 3.502921471822459
13 13.688466322822272
18 18.951607421668996
11 11.651692006191636
11 11.589398231478912
MSE:  1.9824063752323058