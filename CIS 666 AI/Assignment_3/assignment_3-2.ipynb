{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I imported all the necessary tools such as:  <br/>\n",
    "- sent_tokenize, word_tokenize, WordPunctTokenizer for breaking text string into list of sentences or words.\n",
    "- WordNetLemmatizer, PorterStemmer, LancasterStemmer, SnowballStemmer for converting words to it's root form\n",
    "- LatentDirichletAllocation for training and calculating the top 5 words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('.')\n",
    "stop_words.append(',')\n",
    "stop_words.append('\\'s')\n",
    "stop_words.append('\\'\\'')\n",
    "stop_words.append('``')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use nltk.corpus.stopwords to generate stop words. I also decided to add '.', ',', and ''s' becuase the stop word does not have it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = []\n",
    "count = 0\n",
    "for line in open('data.txt', 'r').read().split('\\n'):\n",
    "    count += 1\n",
    "    if line != '':\n",
    "        input_text.append(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data.txt and store each lines in the text file into a list and ignoring the line with no text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for index, line in enumerate(input_text):\n",
    "    word_dict[index] = word_tokenize(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use nltk.tokenize.word_tokenize to break the string text to words for each line and store word_dict with each line numbers as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_word_dict = {}\n",
    "for line in word_dict:\n",
    "    stem_word_dict[line] = []\n",
    "    for i in word_dict[line]:\n",
    "        stem_word_dict[line].append(SnowballStemmer('english').stem(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use nltk.stem.SnowballStemmer to convert words into it's root form this time with SnowballStemmer method instead of having multiple words with the same meaning but are in a different form such as eat, eating, ate, eaten, etc. SnowballStemmer is 'the most reasonable one' but it takes a long time to process and perform task. Since the word for stem is not long I decided to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2\n",
    "bow = {}\n",
    "for i in range(2):\n",
    "    for w in stem_word_dict[i]:\n",
    "        if w not in bow and w not in stop_words:\n",
    "            bow[w] = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for bag of word method from the first 2 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_dict = {}\n",
    "for line in word_dict:\n",
    "    bow_dict[line] = bow.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each list of words I made a copy of the bag of word I created and store them in bow_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in stem_word_dict:\n",
    "    for ww in stem_word_dict[w]:\n",
    "        if ww in bow_dict[w]:\n",
    "            bow_dict[w][ww] += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I count all occurence of each topics that are in bow_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for w in bow_dict:\n",
    "    X.append(list(bow_dict[w].values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I convert all values bow_dict to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "lda.fit(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I called LatentDirichletAllocation. I set the number of conponents to 5 for top 5 words in each topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words of each sentence are:\n",
      "Topic 1 : food\n",
      "Topic 1 : tri\n",
      "Topic 1 : mother\n",
      "Topic 1 : chang\n",
      "Topic 1 : said\n",
      "Topic 2 : angela\n",
      "Topic 2 : lunch\n",
      "Topic 2 : plain\n",
      "Topic 2 : croissant\n",
      "Topic 2 : tri\n",
      "Topic 3 : food\n",
      "Topic 3 : like\n",
      "Topic 3 : tri\n",
      "Topic 3 : sinc\n",
      "Topic 3 : lunch\n",
      "Topic 4 : food\n",
      "Topic 4 : croissant\n",
      "Topic 4 : plain\n",
      "Topic 4 : pasta\n",
      "Topic 4 : like\n",
      "Topic 5 : food\n",
      "Topic 5 : like\n",
      "Topic 5 : tri\n",
      "Topic 5 : sinc\n",
      "Topic 5 : lunch\n"
     ]
    }
   ],
   "source": [
    "words = list(bow.keys())\n",
    "top = 5\n",
    "www = []\n",
    "print('Top 5 words of each sentence are:')\n",
    "for n, topic in enumerate(lda.components_):\n",
    "    sort_topic = topic.argsort()\n",
    "    for i in range(5):\n",
    "        www.append(words[sort_topic[(-1) - i]])\n",
    "        print('Topic', n + 1, ':', words[sort_topic[(-1) - i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words of each sentence are:\n",
      "Topic 1 : The\n",
      "Topic 1 : food\n",
      "Topic 1 : Ciarra\n",
      "Topic 1 : new\n",
      "Topic 1 : said\n",
      "Topic 2 : food\n",
      "Topic 2 : ha\n",
      "Topic 2 : mother\n",
      "Topic 2 : tried\n",
      "Topic 2 : said\n",
      "Topic 3 : food\n",
      "Topic 3 : croissant\n",
      "Topic 3 : plain\n",
      "Topic 3 : pasta\n",
      "Topic 3 : The\n",
      "Topic 4 : lunch\n",
      "Topic 4 : Angela\n",
      "Topic 4 : like\n",
      "Topic 4 : plain\n",
      "Topic 4 : croissant\n",
      "Topic 5 : The\n",
      "Topic 5 : food\n",
      "Topic 5 : Ciarra\n",
      "Topic 5 : new\n",
      "Topic 5 : said\n"
     ]
    }
   ],
   "source": [
    "lemma_word_dict = {}\n",
    "for line in word_dict:\n",
    "    lemma_word_dict[line] = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in word_dict[line]:\n",
    "        lemma_word_dict[line].append(lemmatizer.lemmatize(i, pos='n'))\n",
    "\n",
    "\n",
    "num = 2\n",
    "bow = {}\n",
    "for i in range(2):\n",
    "    for w in lemma_word_dict[i]:\n",
    "        if w not in bow and w not in stop_words:\n",
    "            bow[w] = 0\n",
    "\n",
    "bow_dict = {}\n",
    "for line in word_dict:\n",
    "    bow_dict[line] = bow.copy()\n",
    "\n",
    "for w in lemma_word_dict:\n",
    "    for ww in lemma_word_dict[w]:\n",
    "        if ww in bow_dict[w]:\n",
    "            bow_dict[w][ww] += 1\n",
    "\n",
    "X = []\n",
    "for w in bow_dict:\n",
    "    X.append(list(bow_dict[w].values()))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "# lda.fit(X[:2])\n",
    "lda.fit(X)\n",
    "\n",
    "words = list(bow.keys())\n",
    "top = 5\n",
    "wwww = []\n",
    "print('Top 5 words of each sentence are:')\n",
    "for n, topic in enumerate(lda.components_):\n",
    "    sort_topic = topic.argsort()\n",
    "    for i in range(5):\n",
    "        wwww.append(words[sort_topic[(-1) - i]])\n",
    "        print('Topic', n + 1, ':', words[sort_topic[(-1) - i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I repeat the same steps except that this time I'm using Lemmatizer instead of Stemmer. The result are about the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer  ['food', 'lunch', 'angela', 'pasta', 'tri', 'mother', 'chang', 'said', 'like', 'croissant', 'plain', 'sinc']\n",
      "Stemmer  12\n",
      "Lemmatizer ['food', 'lunch', 'tried', 'pasta', 'ha', 'mother', 'new', 'Ciarra', 'Angela', 'said', 'like', 'croissant', 'plain', 'The']\n",
      "Lemmatizer 14\n"
     ]
    }
   ],
   "source": [
    "print('Stemmer ', list(set(www)))\n",
    "print('Stemmer ', len(list(set(www))))\n",
    "print('Lemmatizer', list(set(wwww)))\n",
    "print('Lemmatizer', len(list(set(wwww))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I count all the unique words in lemmatizer and stemmer and I see that Stemmer has 16 unique words and 17 unique words for lemmatizer.<br/>\n",
    "Stemmer  ['food', 'lunch', 'daili', 'star', 'pasta', 'angela', 'tri', 'new', 'chang', 'said', 'ciarra', 'like', 'croissant', 'plain', 'franco', 'sinc']<br/>\n",
    "Stemmer  16<br/>\n",
    "Lemmatizer ['food', 'lunch', 'tried', 'pasta', 'ha', 'mother', 'new', 'Ciarra', 'Angela', 'said', 'like', 'croissant', 'plain', 'The']<br/>\n",
    "Lemmatizer 14<br/>\n",
    "There are 8 words in each list that match. For all the top 5 contributing words have 5 related words. All topics are related to food."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
