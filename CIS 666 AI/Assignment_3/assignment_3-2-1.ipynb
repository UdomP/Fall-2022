{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = []\n",
    "count = 0\n",
    "for line in open('data.txt', 'r').read().split('\\n'):\n",
    "    count += 1\n",
    "    if line != '':\n",
    "        input_text.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'all', 'know', 'that', 'children', 'are', 'fussy', 'eaters', '-', 'they', 'do', \"n't\", 'like', 'taste', ',', 'shape', ',', 'colour', 'or', 'texture', 'of', 'particular', 'foods', '.', 'But', 'a', 'schoolgirl', 'in', 'England', \"'s\", 'Kent', 'survived', 'on', 'plain', 'pasta', 'and', 'croissants', 'for', 'a', 'decade', ',', 'according', 'to', 'a', 'report', 'in', 'Daily', 'Star', '.', 'The', '13-year-old', ',', 'identified', 'as', 'Ciarra', 'Franco', ',', 'became', 'scared', 'of', 'trying', 'new', 'food', 'items', 'after', 'she', 'almost', 'choked', 'as', 'a', 'toddler', ',', 'the', 'outlet', 'further', 'said', '.', 'Since', 'then', ',', 'the', 'teenager', 'preferred', 'a', 'pack', 'of', 'croissants', 'for', 'lunch', 'and', 'plain', 'pasta', 'for', 'dinner', '.']\n",
      "['Ms', 'Franco', \"'\", 's', 'mother', 'Angela', 'tried', 'very', 'hard', 'to', 'change', 'her', 'food', 'habits', 'and', 'broaden', 'her', 'taste', 'but', 'were', 'unsuccessful', '.', '\"', 'Since', 'she', 'was', 'two', ',', 'one', 'of', 'the', 'only', 'things', 'she', 'has', 'eaten', 'constantly', 'is', 'croissants', 'for', 'lunch', 'and', 'plain', 'pasta', 'for', 'her', 'dinner', ',\"', 'she', 'told', 'Daily', 'Star', '.']\n"
     ]
    }
   ],
   "source": [
    "word_1 = word_tokenize(input_text[0])\n",
    "puncuation_2 = WordPunctTokenizer().tokenize(input_text[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of line 1 and line 2 <br/>\n",
    "I decided to use word and punctuation tokenizer, so that I can see the difference on the final result. I also could not decide which method to use. I decided not to use sentence tokenizer becuase I think the tokenized sentense size are too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_words_1 = []\n",
    "for w in word_1:\n",
    "    snow_words_1.append(SnowballStemmer('english').stem(w))\n",
    "\n",
    "snow_words_2 = []\n",
    "for w in puncuation_2:\n",
    "    snow_words_2.append(SnowballStemmer('english').stem(w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming <br/>\n",
    "SnowballStemmer is the \"Most reasonable one to use\", but the only catch is the decrease in strictness and speed. Since I'm only stemming only two lines with not too many words in it the high computation will not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_1_noun = []\n",
    "for w in word_1:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_1_noun.append(lemmatizer.lemmatize(w, pos='n'))\n",
    "\n",
    "lemma_2_noun = []\n",
    "for w in puncuation_2:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_2_noun.append(lemmatizer.lemmatize(w, pos='n'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization <br/>\n",
    "I want to see how lemmatization and stemming perform, so I have two sets of words in base/root form that I will be testing and comparing how they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoW_lemma = {}\n",
    "BoW_stem = {}\n",
    "\n",
    "for w in lemma_1_noun:\n",
    "    if w not in BoW_lemma:\n",
    "        BoW_lemma[w] = 0\n",
    "\n",
    "for w in lemma_2_noun:\n",
    "    if w not in BoW_lemma:\n",
    "        BoW_lemma[w] = 0\n",
    "\n",
    "for w in snow_words_1:\n",
    "    if w not in BoW_stem:\n",
    "        BoW_stem[w] = 0\n",
    "\n",
    "for w in snow_words_2:\n",
    "    if w not in BoW_stem:\n",
    "        BoW_stem[w] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bow_lemma_1 = BoW_lemma.copy()\n",
    "Bow_lemma_2 = BoW_lemma.copy()\n",
    "\n",
    "for w1 in lemma_1_noun:\n",
    "    Bow_lemma_1[w1] = 1\n",
    "for w2 in lemma_2_noun:\n",
    "    Bow_lemma_2[w2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'We': 1, 'all': 1, 'know': 1, 'that': 1, 'child': 1, 'are': 1, 'fussy': 1, 'eater': 1, '-': 1, 'they': 1, 'do': 1, \"n't\": 1, 'like': 1, 'taste': 1, ',': 1, 'shape': 1, 'colour': 1, 'or': 1, 'texture': 1, 'of': 1, 'particular': 1, 'food': 1, '.': 1, 'But': 1, 'a': 1, 'schoolgirl': 1, 'in': 1, 'England': 1, \"'s\": 1, 'Kent': 1, 'survived': 1, 'on': 1, 'plain': 1, 'pasta': 1, 'and': 1, 'croissant': 1, 'for': 1, 'decade': 1, 'according': 1, 'to': 1, 'report': 1, 'Daily': 1, 'Star': 1, 'The': 1, '13-year-old': 1, 'identified': 1, 'Ciarra': 1, 'Franco': 1, 'became': 1, 'scared': 1, 'trying': 1, 'new': 1, 'item': 1, 'after': 1, 'she': 1, 'almost': 1, 'choked': 1, 'toddler': 1, 'the': 1, 'outlet': 1, 'further': 1, 'said': 1, 'Since': 1, 'then': 1, 'teenager': 1, 'preferred': 1, 'pack': 1, 'lunch': 1, 'dinner': 1, 'Ms': 0, \"'\": 0, 's': 0, 'mother': 0, 'Angela': 0, 'tried': 0, 'very': 0, 'hard': 0, 'change': 0, 'her': 0, 'habit': 0, 'broaden': 0, 'but': 0, 'were': 0, 'unsuccessful': 0, '\"': 0, 'wa': 0, 'two': 0, 'one': 0, 'only': 0, 'thing': 0, 'ha': 0, 'eaten': 0, 'constantly': 0, 'is': 0, ',\"': 0, 'told': 0}\n",
      "{'We': 0, 'all': 0, 'know': 0, 'that': 0, 'child': 0, 'are': 0, 'fussy': 0, 'eater': 0, '-': 0, 'they': 0, 'do': 0, \"n't\": 0, 'like': 0, 'taste': 1, ',': 1, 'shape': 0, 'colour': 0, 'or': 0, 'texture': 0, 'of': 1, 'particular': 0, 'food': 1, '.': 1, 'But': 0, 'a': 0, 'schoolgirl': 0, 'in': 0, 'England': 0, \"'s\": 0, 'Kent': 0, 'survived': 0, 'on': 0, 'plain': 1, 'pasta': 1, 'and': 1, 'croissant': 1, 'for': 1, 'decade': 0, 'according': 0, 'to': 1, 'report': 0, 'Daily': 1, 'Star': 1, 'The': 0, '13-year-old': 0, 'identified': 0, 'Ciarra': 0, 'Franco': 1, 'became': 0, 'scared': 0, 'trying': 0, 'new': 0, 'item': 0, 'after': 0, 'she': 1, 'almost': 0, 'choked': 0, 'toddler': 0, 'the': 1, 'outlet': 0, 'further': 0, 'said': 0, 'Since': 1, 'then': 0, 'teenager': 0, 'preferred': 0, 'pack': 0, 'lunch': 1, 'dinner': 1, 'Ms': 1, \"'\": 1, 's': 1, 'mother': 1, 'Angela': 1, 'tried': 1, 'very': 1, 'hard': 1, 'change': 1, 'her': 1, 'habit': 1, 'broaden': 1, 'but': 1, 'were': 1, 'unsuccessful': 1, '\"': 1, 'wa': 1, 'two': 1, 'one': 1, 'only': 1, 'thing': 1, 'ha': 1, 'eaten': 1, 'constantly': 1, 'is': 1, ',\"': 1, 'told': 1}\n"
     ]
    }
   ],
   "source": [
    "print(Bow_lemma_1)\n",
    "print(Bow_lemma_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'We': 0, 'Ms': 0, 'all': 0, 'Franco': 0, 'know': 0, \"'\": 0, 'that': 0, 's': 0, 'child': 0, 'mother': 0, 'are': 0, 'Angela': 0, 'fussy': 0, 'tried': 0, 'eater': 0, 'very': 0, '-': 0, 'hard': 0, 'they': 0, 'to': 0, 'do': 0, 'change': 0, \"n't\": 0, 'her': 0, 'like': 0, 'food': 0, 'taste': 0, 'habit': 0, ',': 0, 'and': 0, 'shape': 0, 'broaden': 0, 'colour': 0, 'or': 0, 'but': 0, 'texture': 0, 'were': 0, 'of': 0, 'unsuccessful': 0, 'particular': 0, '.': 0, '\"': 0, 'Since': 0, 'But': 0, 'she': 0, 'a': 0, 'wa': 0, 'schoolgirl': 0, 'two': 0, 'in': 0, 'England': 0, 'one': 0, \"'s\": 0, 'Kent': 0, 'the': 0, 'survived': 0, 'only': 0, 'on': 0, 'thing': 0, 'plain': 0, 'pasta': 0, 'ha': 0, 'eaten': 0, 'croissant': 0, 'constantly': 0, 'for': 0, 'is': 0, 'decade': 0, 'lunch': 0, 'according': 0, 'report': 0, 'Daily': 0, 'dinner': 0, 'Star': 0, ',\"': 0, 'The': 0, 'told': 0, '13-year-old': 0, 'identified': 0}\n",
      "{'we': 0, 'ms': 0, 'all': 0, 'franco': 0, 'know': 0, \"'\": 0, 'that': 0, 's': 0, 'children': 0, 'mother': 0, 'are': 0, 'angela': 0, 'fussi': 0, 'tri': 0, 'eater': 0, 'veri': 0, '-': 0, 'hard': 0, 'they': 0, 'to': 0, 'do': 0, 'chang': 0, \"n't\": 0, 'her': 0, 'like': 0, 'food': 0, 'tast': 0, 'habit': 0, ',': 0, 'and': 0, 'shape': 0, 'broaden': 0, 'colour': 0, 'or': 0, 'but': 0, 'textur': 0, 'were': 0, 'of': 0, 'unsuccess': 0, 'particular': 0, '.': 0, '\"': 0, 'sinc': 0, 'she': 0, 'a': 0, 'was': 0, 'schoolgirl': 0, 'two': 0, 'in': 0, 'england': 0, 'one': 0, \"'s\": 0, 'kent': 0, 'the': 0, 'surviv': 0, 'onli': 0, 'on': 0, 'thing': 0, 'plain': 0, 'pasta': 0, 'has': 0, 'eaten': 0, 'croissant': 0, 'constant': 0, 'for': 0, 'is': 0, 'decad': 0, 'lunch': 0, 'accord': 0, 'report': 0, 'daili': 0, 'dinner': 0, 'star': 0, ',\"': 0, 'told': 0, '13-year-old': 0, 'identifi': 0}\n"
     ]
    }
   ],
   "source": [
    "# threshold = 0.8\n",
    "# num_lemma = int(threshold * len(BoW_lemma))\n",
    "# num_stem = int(threshold * len(BoW_stem))\n",
    "\n",
    "# lemma_train = lemma_1_noun[:num_lemma] + lemma_2_noun[:num_lemma]\n",
    "# lemma_test = lemma_1_noun[num_lemma:] + lemma_2_noun[num_lemma:]\n",
    "\n",
    "# stem_train = snow_words_1[:num_stem] + snow_words_2[:num_stem]\n",
    "# stem_test = snow_words_1[num_stem:] + snow_words_2[num_stem:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
