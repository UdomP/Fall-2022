{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb1287f",
   "metadata": {},
   "source": [
    "# Level-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094ac6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    "\n",
    "# Extract last N letters from the input word\n",
    "# and that will act as our \"feature\"\n",
    "def extract_features(word, N):\n",
    "    letters = word[:]\n",
    "    return {'feature': letters.lower()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1476afb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry ==> male\n",
      "Hermione ==> female\n",
      "Kate ==> female\n",
      "Ron ==> male\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    # Create training data using labeled names available in NLTK\n",
    "    male_list = [(name, 'male') for name in names.words('male.txt')]\n",
    "    female_list = [(name, 'female') for name in names.words('female.txt')]\n",
    "    data = (male_list + female_list)\n",
    "\n",
    "    \n",
    "    # Create test data\n",
    "    input_names = ['Harry', 'Hermione', 'Kate', 'Ron']\n",
    "\n",
    "    # Define the number of samples used for train and test\n",
    "    num_train = int(0.8 * len(data))\n",
    "    features = [(extract_features(n, 8), gender) for (n, gender) in data]\n",
    "    train_data, test_data = features[:num_train], features[num_train:]\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "    # Predict outputs for input names using the trained classifier model\n",
    "    for name in input_names:\n",
    "        print(name, '==>', classifier.classify(extract_features(name, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ef036",
   "metadata": {},
   "source": [
    "# Level-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afac246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
    "\n",
    "f=open('category_check.txt','r',errors='ignore')\n",
    "input_text=f.read()\n",
    "input_data=sent_tokenize(input_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685d7c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: Tesla is offering multiple models for cheaper prices. \n",
      "Predicted category: Autos\n",
      "\n",
      "Input: ChatGPT is an chatbot developed by OpenAI which is built on the ground of Natural Language Processing and Machine Learning. \n",
      "Predicted category: Artificial Intelligence\n",
      "\n",
      "Input: Players need to be careful when they are close to goal posts. \n",
      "Predicted category: Hockey\n",
      "\n",
      "Input: Political debates help us understand the perspectives of both sides. \n",
      "Predicted category: Politics\n",
      "\n",
      "Input: One of the most remarkable use of the newly developed Bioinformatics field is the rapid invention of Covid-19 vaccine. \n",
      "Predicted category: Medicine\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the category map\n",
    "category_map = {'talk.politics.misc': 'Politics', 'rec.autos': 'Autos', \n",
    "        'rec.sport.hockey': 'Hockey', 'sci.electronics': 'Artificial Intelligence', \n",
    "        'sci.med': 'Medicine'}\n",
    "\n",
    "# Get the training dataset\n",
    "training_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=5)\n",
    "\n",
    "# Build a count vectorizer and extract term counts \n",
    "count_vectorizer = CountVectorizer()\n",
    "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
    "\n",
    "# Create the tf-idf transformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)\n",
    "\n",
    "\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
    "\n",
    "# Transform input data using count vectorizer\n",
    "input_tc = count_vectorizer.transform(input_data)\n",
    "\n",
    "# Transform vectorized data using tfidf transformer\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "\n",
    "# Predict the output categories\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "\n",
    "# Print the outputs\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print('\\nInput:', sent, '\\nPredicted category:', category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e34e53",
   "metadata": {},
   "source": [
    "# Level-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6608fe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movie review predictions:\n",
      "\n",
      "Review: The actors in this movie were great\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.56\n",
      "\n",
      "Review: This is such an idiotic movie. I will not recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.87\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    " \n",
    "    \n",
    "# Extract features from the input list of words\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "if __name__=='__main__':\n",
    "    # Load the reviews from the corpus \n",
    "    fileids_pos = movie_reviews.fileids('pos')\n",
    "    fileids_neg = movie_reviews.fileids('neg')\n",
    "     \n",
    "    # Extract the features from the reviews\n",
    "    features_pos = [(extract_features(movie_reviews.words(fileids=[f])), 'Positive') for f in fileids_pos]\n",
    "    features_neg = [(extract_features(movie_reviews.words(fileids=[f])), 'Negative') for f in fileids_neg]\n",
    "     \n",
    "    # Define the train and test split (80% and 20%)\n",
    "    threshold = 0.8\n",
    "    num_pos = int(threshold * len(features_pos))\n",
    "    num_neg = int(threshold * len(features_neg))\n",
    "     \n",
    "     # Create training and training datasets\n",
    "    features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
    "    features_test = features_pos[num_pos:] + features_neg[num_neg:]  \n",
    "\n",
    "    # Train a Naive Bayes classifier \n",
    "    classifier = NaiveBayesClassifier.train(features_train)\n",
    "    \n",
    "    input_reviews = [\n",
    "        'The actors in this movie were great', \n",
    "        'This is such an idiotic movie. I will not recommend it to anyone.' \n",
    "    ]\n",
    "\n",
    "    print(\"\\nMovie review predictions:\")\n",
    "    for review in input_reviews:\n",
    "        print(\"\\nReview:\", review)\n",
    "\n",
    "        # Compute the probabilities\n",
    "        probabilities = classifier.prob_classify(extract_features(review.split()))\n",
    "\n",
    "        # Pick the maximum value\n",
    "        predicted_sentiment = probabilities.max()\n",
    "\n",
    "        # Print outputs\n",
    "        print(\"Predicted sentiment:\", predicted_sentiment)\n",
    "        print(\"Probability:\", round(probabilities.prob(predicted_sentiment), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873edc2",
   "metadata": {},
   "source": [
    "# Level-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dac58d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 contributing words to each topic:\n",
      "\n",
      "Topic 0\n",
      "\"movi\" ==> 7.1%\n",
      "\"amaz\" ==> 7.1%\n",
      "\"terribl\" ==> 7.1%\n",
      "\"except\" ==> 7.1%\n",
      "\"director\" ==> 7.1%\n",
      "\n",
      "Topic 1\n",
      "\"movi\" ==> 11.4%\n",
      "\"find\" ==> 6.8%\n",
      "\"peopl\" ==> 6.8%\n",
      "\"great\" ==> 6.8%\n",
      "\"storytel\" ==> 6.8%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "\n",
    "# Load input data\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Processor function for tokenizing, removing stop \n",
    "# words, and stemming\n",
    "def process(input_text):\n",
    "    # Create a regular expression tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Create a Snowball stemmer \n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # Get the list of stop words \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "\n",
    "    # Remove the stop words \n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    \n",
    "    # Perform stemming on the tokenized words \n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "\n",
    "    return tokens_stemmed\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    # Load input data\n",
    "    data = load_data('movie_review.txt')\n",
    "\n",
    "    # Create a list for sentence tokens\n",
    "    tokens = [process(x) for x in data]\n",
    "\n",
    "    # Create a dictionary based on the sentence tokens \n",
    "    dict_tokens = corpora.Dictionary(tokens)\n",
    "        \n",
    "    # Create a document-term matrix\n",
    "    doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]\n",
    "\n",
    "    # Define the number of topics for the LDA model\n",
    "\n",
    "    # Generate the LDA model \n",
    "    ldamodel = models.ldamodel.LdaModel(doc_term_mat, 2, id2word=dict_tokens, passes=25)\n",
    "\n",
    "    num_words = 5\n",
    "    print('\\nTop ' + str(num_words) + ' contributing words to each topic:')\n",
    "    for item in ldamodel.print_topics(num_topics=2, num_words=num_words):\n",
    "        print('\\nTopic', item[0])\n",
    "\n",
    "        # Print the contributing words along with their relative contributions \n",
    "        list_of_strings = item[1].split(' + ')\n",
    "        for text in list_of_strings:\n",
    "            weight = text.split('*')[0]\n",
    "            word = text.split('*')[1]\n",
    "            print(word, '==>', str(round(float(weight) * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fdc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
