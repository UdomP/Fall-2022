[357 rows x 33 columns]
(357, 26)
X_train:  (321, 26)
y_train:  (321,)
X_test:  (36, 26)
y_test:  (36,)
11.523809523809524
Iteration 1, loss = 10.13624269
Iteration 2, loss = 0.93878869
Iteration 3, loss = 0.84194341
Iteration 4, loss = 0.79778561
Iteration 5, loss = 0.73669647
Iteration 6, loss = 0.63704081
Iteration 7, loss = 0.59260346
Iteration 8, loss = 0.62019443
Iteration 9, loss = 0.55909307
Iteration 10, loss = 0.52752401
Iteration 11, loss = 0.51875196
Iteration 12, loss = 0.51166683
Iteration 13, loss = 0.50282683
Iteration 14, loss = 0.49888484
Iteration 15, loss = 0.48403574
Iteration 16, loss = 0.47771687
Iteration 17, loss = 0.47234407
Iteration 18, loss = 0.47048689
Iteration 19, loss = 0.46534420
Iteration 20, loss = 0.45890790
Iteration 21, loss = 0.46549539
Iteration 22, loss = 0.44564786
Iteration 23, loss = 0.45440546
Iteration 24, loss = 0.44624997
Iteration 25, loss = 0.44411807
Iteration 26, loss = 0.43572013
Iteration 27, loss = 0.43409166
Iteration 28, loss = 0.44172996
Iteration 29, loss = 0.42090479
Iteration 30, loss = 0.41778427
Iteration 31, loss = 0.41729811
Iteration 32, loss = 0.42360236
Iteration 33, loss = 0.41211826
Iteration 34, loss = 0.40961945
Iteration 35, loss = 0.41350873
Iteration 36, loss = 0.41469520
Iteration 37, loss = 0.41531120
Iteration 38, loss = 0.40442359
Iteration 39, loss = 0.39875918
Iteration 40, loss = 0.40127498
Iteration 41, loss = 0.39255229
Iteration 42, loss = 0.39319715
Iteration 43, loss = 0.39579695
Iteration 44, loss = 0.39587677
Iteration 45, loss = 0.38953292
Iteration 46, loss = 0.39577785
Iteration 47, loss = 0.38876403
Iteration 48, loss = 0.39567248
Iteration 49, loss = 0.38607507
Iteration 50, loss = 0.39281776
Iteration 51, loss = 0.38594166
Iteration 52, loss = 0.38108920
Iteration 53, loss = 0.39529067
Iteration 54, loss = 0.38068162
Iteration 55, loss = 0.37303718
Iteration 56, loss = 0.36888380
Iteration 57, loss = 0.37882619
Iteration 58, loss = 0.37607352
Iteration 59, loss = 0.36643669
Iteration 60, loss = 0.36449832
Iteration 61, loss = 0.37123209
Iteration 62, loss = 0.36369851
Iteration 63, loss = 0.36056278
Iteration 64, loss = 0.37446446
Iteration 65, loss = 0.38398490
Iteration 66, loss = 0.36858174
Iteration 67, loss = 0.36190254
Iteration 68, loss = 0.35489582
Iteration 69, loss = 0.35616201
Iteration 70, loss = 0.35495796
Iteration 71, loss = 0.36100418
Iteration 72, loss = 0.36315967
Iteration 73, loss = 0.35024680
Iteration 74, loss = 0.35012579
Iteration 75, loss = 0.35042055
Iteration 76, loss = 0.35671346
Iteration 77, loss = 0.35196394
Iteration 78, loss = 0.34615105
Iteration 79, loss = 0.35265952
Iteration 80, loss = 0.35095319
Iteration 81, loss = 0.34343442
Iteration 82, loss = 0.36276650
Iteration 83, loss = 0.37570168
Iteration 84, loss = 0.34094658
Iteration 85, loss = 0.34256667
Iteration 86, loss = 0.34530753
Iteration 87, loss = 0.34986231
Iteration 88, loss = 0.34535013
Iteration 89, loss = 0.33795210
Iteration 90, loss = 0.34396046
Iteration 91, loss = 0.33706570
Iteration 92, loss = 0.33662927
Iteration 93, loss = 0.33624074
Iteration 94, loss = 0.35327945
Iteration 95, loss = 0.33386199
Iteration 96, loss = 0.33761623
Iteration 97, loss = 0.33518005
Iteration 98, loss = 0.34046060
Iteration 99, loss = 0.33078529
Iteration 100, loss = 0.34514620
C:\Users\serey\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.
  warnings.warn(
8 8.36980406885955
7 7.756318446475584
15 15.298365854760911
8 7.657561072266564
15 15.273190112993378
13 11.878079889500327
13 11.660886427401625
8 6.610905640988527
7 6.504289454410439
15 15.325773759702008
11 11.952267779241916
8 8.212394381437022
15 14.08027464074512
5 5.784787002089805
11 12.185989652065707
16 16.00930122145414
13 12.685470159110157
16 15.732125716169925
14 13.581369135038358
8 8.15705343085224
9 8.939934398225638
10 10.966812049069986
10 9.45103230074965
5 5.1566477103218595
9 9.559801619897867
12 12.317758655637185
11 10.304210693254012
10 11.241203333390597
14 13.51091969708326
8 7.594383770480788
13 12.125989376268905
8 8.481109048965724
16 16.162051263333232
10 10.968276979002686
13 13.541715583720777
8 8.925800923217249
MSE:  0.492522150568979